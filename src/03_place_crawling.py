# ===== 1ï¸âƒ£ í™˜ê²½ ì„¸íŒ… (Colab í•œì •) =====
# !apt-get update -qq
# !apt-get install -y unzip > /dev/null 2>&1
# !wget -q https://storage.googleapis.com/chrome-for-testing-public/131.0.6778.85/linux64/chrome-linux64.zip
# !wget -q https://storage.googleapis.com/chrome-for-testing-public/131.0.6778.85/linux64/chromedriver-linux64.zip
# !unzip -q chrome-linux64.zip
# !unzip -q chromedriver-linux64.zip
# !mv chrome-linux64 /usr/local/chrome
# !mv chromedriver-linux64/chromedriver /usr/local/bin/chromedriver
# !chmod +x /usr/local/bin/chromedriver
# !pip install selenium==4.25.0 pandas -q


# ===== 2ï¸âƒ£ ë“œë¼ì´ë²„ ë° ê³µí†µ í•¨ìˆ˜ =====
import pandas as pd, re, time
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

chrome_service = Service('/usr/local/bin/chromedriver')
chrome_options = Options()
chrome_options.binary_location = '/usr/local/chrome/chrome'
chrome_options.add_argument('--headless')
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')
chrome_options.add_argument('--window-size=1920,1080')

driver = webdriver.Chrome(service=chrome_service, options=chrome_options)
wait = WebDriverWait(driver, 15)
print("âœ… ChromeDriver ì‹¤í–‰ ì™„ë£Œ")

def switch_left():
    """ì™¼ìª½ ê²€ìƒ‰ê²°ê³¼ iframe ì „í™˜"""
    driver.switch_to.default_content()
    wait.until(EC.frame_to_be_available_and_switch_to_it((By.ID, "searchIframe")))

def switch_right():
    """ì˜¤ë¥¸ìª½ ìƒì„¸ì •ë³´ iframe ì „í™˜"""
    driver.switch_to.default_content()
    wait.until(EC.frame_to_be_available_and_switch_to_it((By.ID, "entryIframe")))

# ===== 3ï¸âƒ£ ìƒì„¸í˜ì´ì§€ í¬ë¡¤ëŸ¬ (ë©”ë‰´, ë¦¬ë·°, ë³„ì , í¸ì˜ì‹œì„¤) =====
def crawl_place_details(driver):
    """í˜„ì¬ entryIframe ì•ˆì—ì„œ ë©”ë‰´, í¸ì˜ì‹œì„¤, ë¦¬ë·°, ë³„ì ì„ ëª¨ë‘ í¬ë¡¤ë§"""
    data = {
        "menus": [],
        "facilities": [],
        "reviews": [],
        "rating": None
    }

    # â­ ë³„ì 
    try:
        data["rating"] = driver.find_element(By.CLASS_NAME, "PXMot").text.strip()
    except:
        data["rating"] = None

    # ğŸ½ ë©”ë‰´ + ê°€ê²©
    try:
        menu_section = driver.find_element(
            By.XPATH, "//div[contains(@class,'place_section') and .//div[text()='ë©”ë‰´']]"
        )
        menu_items = menu_section.find_elements(By.TAG_NAME, "li")
        for item in menu_items:
            try:
                name = item.find_element(By.XPATH, ".//a[contains(@href, '/menu/')]").text.strip()
            except:
                name = None
            try:
                price = item.find_element(By.XPATH, ".//div[contains(text(),'ì›')]").text.strip()
            except:
                price = None
            if name or price:
                data["menus"].append(f"{name} ({price})" if price else name)
    except:
        data["menus"] = []

    # ğŸª í¸ì˜ì‹œì„¤
    try:
        facilities_section = driver.find_element(
            By.XPATH, "//div[contains(@class,'place_section') and .//div[contains(text(),'í¸ì˜ì‹œì„¤')]]"
        )
        facility_items = facilities_section.find_elements(By.XPATH, ".//span")
        for f in facility_items:
            text = f.text.strip()
            if text:
                data["facilities"].append(text)
    except:
        data["facilities"] = []

    # ğŸ’¬ ë°©ë¬¸ì ë¦¬ë·°
    try:
        review_section = driver.find_element(
            By.XPATH, "//div[contains(@class,'place_section') and .//div[contains(text(),'ë¦¬ë·°')]]"
        )
        driver.execute_script("arguments[0].scrollIntoView(true);", review_section)
        time.sleep(1.5)
        review_texts = driver.find_elements(By.XPATH, "//span[contains(@class,'zPfVt')]")
        for r in review_texts:
            text = r.text.strip()
            if text and len(text) > 3:
                data["reviews"].append(text)
    except:
        data["reviews"] = []

    return data

# ===== 4ï¸âƒ£ ë©”ì¸ í¬ë¡¤ëŸ¬ (ê²€ìƒ‰ â†’ ìƒì„¸ ì§„ì… â†’ ë°ì´í„° ìˆ˜ì§‘) =====
def crawl_store_info(name, sig, emd):
    # ğŸ”¹ "ì " ì˜ˆì™¸ì²˜ë¦¬
    query = name if ("ì " in name and "ë°˜ì " not in name) else f"{name} {sig} {emd}"

    print(f"ğŸ” ê²€ìƒ‰ ì¤‘: {query}")
    result = {
        "restaurant_name": name,
        "sig_kor_nm": sig,
        "emd_kor_nm": emd,
        "place_id": None,
        "rating": None,
        "menus": None,
        "facilities": None,
        "reviews": None
    }

    try:
        driver.get("https://map.naver.com/v5/search/" + query)
        time.sleep(2)

        # âœ… CASE 1: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ ì¡´ì¬
        try:
            switch_left()
            items = driver.find_elements(By.XPATH, '//*[@id="_pcmap_list_scroll_container"]/ul/li')
            if items:
                items[0].find_element(By.TAG_NAME, "a").send_keys(Keys.ENTER)
                switch_right()
        except:
            # âœ… CASE 2: ë°”ë¡œ ìƒì„¸í˜ì´ì§€ë¡œ ì§„ì…
            print("â„¹ï¸ ë¦¬ìŠ¤íŠ¸ ì—†ì´ ìƒì„¸í˜ì´ì§€ë¡œ ë°”ë¡œ ì´ë™ ê°ì§€")
            switch_right()

        # ===== ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ =====
        current_url = driver.current_url
        if m := re.search(r'place/(\d+)', current_url):
            result["place_id"] = m.group(1)

        details = crawl_place_details(driver)
        result.update({
            "rating": details["rating"],
            "menus": ", ".join(details["menus"]) if details["menus"] else None,
            "facilities": ", ".join(details["facilities"]) if details["facilities"] else None,
            "reviews": " | ".join(details["reviews"][:5]) if details["reviews"] else None
        })

        print(f"âœ… ì™„ë£Œ: {name} (placeId={result['place_id']})")
        return result

    except Exception as e:
        print(f"âš ï¸ ì˜ˆì™¸ ë°œìƒ: {e}")
        return result


# ===== 5ï¸âƒ£ ì „ì²´ CSV ì‹¤í–‰ ë° ì €ì¥ =====
df = pd.read_csv("good_restaurant_temp.csv", encoding="utf-8")
print(f"ğŸ“„ ì´ {len(df)}ê°œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")

results = []
for i, row in df.iterrows():
    res = crawl_store_info(row["restaurant_name"], row["sig_kor_nm"], row["emd_kor_nm"])
    results.append(res)

output = pd.DataFrame(results)
output.to_csv("good_restaurant_detail.csv", index=False, encoding="utf-8-sig")
print("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ â†’ good_restaurant_detail.csv ì €ì¥ ì™„ë£Œ")

driver.quit()
